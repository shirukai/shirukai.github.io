
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>StructuredStreaming动态更新参数 - Rukey</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="TriDiamond Obsidian,"> 
    <meta name="description" content="
版本说明：Spark 2.4

1 前言在使用StructuredStreaming的时候，我们可能会遇到在不重启Spark应用的情况下动态的更新参数，如：动态更新某个过滤条件、动态更新分区数量、,"> 
    <meta name="author" content="shirukai"> 
    <link rel="alternative" href="atom.xml" title="Rukey" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link href="https://fonts.loli.net/css?family=Roboto+Mono|Rubik&display=swap" rel="stylesheet">
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">

    
<link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.7.2/animate.min.css">

    
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">

    
<link rel="stylesheet" href="//cdn.bootcss.com/codemirror/5.48.4/codemirror.min.css">

    
<link rel="stylesheet" href="//cdn.bootcss.com/codemirror/5.48.4/theme/dracula.css">

    
<link rel="stylesheet" href="/css/obsidian.css">

    
<link rel="stylesheet" href="/css/ball-atom.min.css">

<meta name="generator" content="Hexo 5.4.0"></head>


<body class="loading">
    <div class="loader">
        <div class="la-ball-atom la-2x">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
    <span id="config-title" style="display:none">Rukey</span>
    <div id="loader"></div>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation animated fadeIn fast delay-1s">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="https://shirukai.github.io">
        <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div>
        <h3 class="subtitle">StructuredStreaming动态更新参数</h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                        <a href="javascript:;" class="iconfont icon-share1"></a>
                        <div class="share-component-cc" data-disabled="facebook,douban,linkedin,diandian,tencent,google"></div>
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class=article-header-wrapper>
    <div class="article-header">
        <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/img/covers/8.jpg);">
        </div>
        <div class="else">
            <p class="animated fadeInDown">
                
                <a href="/categories/Spark"><b>「
                    </b>SPARK<b> 」</b></a>
                
                August 24, 2020
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/blog/structuredstreaming-dynamic-update-parameters.html" title="StructuredStreaming动态更新参数" class="">StructuredStreaming动态更新参数</a>
            </h3>
            
            <p class="post-count animated fadeInDown">
                
                <span>
                    <b class="iconfont icon-text2"></b> <i>Words count</i>
                    49k
                </span>
                
                
                <span>
                    <b class="iconfont icon-timer__s"></b> <i>Reading time</i>
                    45 mins.
                </span>
                
                
                
                <span id="busuanzi_container_page_pv">
                    <b class="iconfont icon-read"></b> <i>Read count</i>
                    <span id="busuanzi_value_page_pv">0</span>
                </span>
                
            </p>
            
            
            <ul class="animated fadeInDown post-tags-list" itemprop="keywords"><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn">
            <blockquote>
<p>版本说明：Spark 2.4</p>
</blockquote>
<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>在使用StructuredStreaming的时候，我们可能会遇到在不重启Spark应用的情况下动态的更新参数，如：动态更新某个过滤条件、动态更新分区数量、动态更新join的静态数据等。在工作中，遇到了一个应用场景，是实时数据与静态DataFrame去Join，然后做一些处理，但是这个静态DataFrame偶尔会发生变化，要求在不重启Spark应用的前提下去动态更新。目前总结了两种解决方案，一种是基于重写数据源的动态更新，另一种是重启Query的动态更新，下面将分别介绍下两种方案的实现。</p>
<h1 id="2-基于重写数据源的动态更新"><a href="#2-基于重写数据源的动态更新" class="headerlink" title="2 基于重写数据源的动态更新"></a>2 基于重写数据源的动态更新</h1><h2 id="2-1-应用场景"><a href="#2-1-应用场景" class="headerlink" title="2.1 应用场景"></a>2.1 应用场景</h2><p>此方案仅适用于实时数据与离线数据(数据量不大)join，但离线数据会发生更新的情况。</p>
<h2 id="2-2-实现思路"><a href="#2-2-实现思路" class="headerlink" title="2.2 实现思路"></a>2.2 实现思路</h2><p>此方案的实现思路是在Spark读取实时数据的同时，检查离线数据是否更新，如果发生更新，将更新数据写入实时数据的RDD，然后生成DataFrame。如果没发生更新从缓存中取离线数据写入实时数据的RDD，然后生成DataFrame。</p>
<p><img src="https://cdn.jsdelivr.net/gh/shirukai/images/c3058b85563d22b9d746f5a81160e0a7.jpg"></p>
<h2 id="2-3-Demo演示"><a href="#2-3-Demo演示" class="headerlink" title="2.3 Demo演示"></a>2.3 Demo演示</h2><p>这里以Kafka实时数据、REST请求离线数据为例进行动态更新演示。</p>
<h3 id="2-3-1-数据描述"><a href="#2-3-1-数据描述" class="headerlink" title="2.3.1 数据描述"></a>2.3.1 数据描述</h3><h4 id="Kafka数据格式"><a href="#Kafka数据格式" class="headerlink" title="Kafka数据格式"></a>Kafka数据格式</h4><pre><code class="json">[
    &#123;
        &quot;namespace&quot;: &quot;000003&quot;,
        &quot;internalSeriesId&quot;: &quot;hiacloud0003000094L[]&quot;,
        &quot;regions&quot;: 10,
        &quot;v&quot;: &quot;F#100.90&quot;,
        &quot;s&quot;: 0,
        &quot;t&quot;: 1550193281708,
        &quot;gatewayId&quot;: &quot;hiacloud&quot;,
        &quot;pointId&quot;: &quot;000003F&quot;
    &#125;
]
</code></pre>
<h4 id="REST离线数据格式"><a href="#REST离线数据格式" class="headerlink" title="REST离线数据格式"></a>REST离线数据格式</h4><pre><code class="json">&#123;
    &quot;code&quot;: 0,
    &quot;msg&quot;: &quot;操作成功&quot;,
    &quot;data&quot;: [
        &#123;
            &quot;instanceId&quot;: &quot;1&quot;,
            &quot;instanceParams&quot;: &quot;[&#123;\&quot;label\&quot;:\&quot;上限值\&quot;,\&quot;name\&quot;:\&quot;upperbound\&quot;,\&quot;value\&quot;:\&quot;100.823\&quot;&#125;,&#123;\&quot;label\&quot;:\&quot;下限值\&quot;,\&quot;name\&quot;:\&quot;lowerbound\&quot;,\&quot;value\&quot;:\&quot;50.534\&quot;&#125;]&quot;,
            &quot;pointId&quot;: [
                &quot;hiacloud/000000F&quot;,
                &quot;hiacloud/000002F&quot;,
                &quot;hiacloud/000001F&quot;
            ]
        &#125;
    ]
&#125;
</code></pre>
<h4 id="最终生成DataFrame格式"><a href="#最终生成DataFrame格式" class="headerlink" title="最终生成DataFrame格式"></a>最终生成DataFrame格式</h4><pre><code class="shell">+---------+------+---+-----------------------+---------+-------+----------+----------+
|namespace|v     |s  |t                      |gatewayId|pointId|lowerbound|upperbound|
+---------+------+---+-----------------------+---------+-------+----------+----------+
|000000   |108.79|0  |2019-02-15 09:34:19.985|hiacloud |000000F|50.534    |100.823   |
|000001   |108.79|0  |2019-02-15 09:34:19.985|hiacloud |000001F|50.534    |100.823   |
|000002   |108.79|0  |2019-02-15 09:34:19.985|hiacloud |000002F|50.534    |100.823   |
|000003   |108.78|0  |2019-02-15 09:34:18.856|hiacloud |000003F|          |          |
|000003   |108.79|0  |2019-02-15 09:34:19.985|hiacloud |000003F|          |          |
+---------+------+---+-----------------------+---------+-------+----------+----------+
</code></pre>
<h3 id="2-3-2-自定义KafkaSource"><a href="#2-3-2-自定义KafkaSource" class="headerlink" title="2.3.2 自定义KafkaSource"></a>2.3.2 自定义KafkaSource</h3><p>为了实现上述效果，我们需要自定义KafkaSource，关于如何重写数据源可以参考《<a href="https://shirukai.github.io/2019/01/25/StructuredStreaming%20%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%BA%90%E5%8F%8A%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90/">StructuredStreaming内置数据源及自定义数据源</a>》，这里我们大体分三步来实现：</p>
<ul>
<li><p>第一步：创建Kafka RDD处理类</p>
</li>
<li><p>第二步：重写Source</p>
</li>
<li><p>第三步：重写Provider</p>
</li>
</ul>
<h4 id="2-3-2-1-创建KafkaRDD处理类：KafkaRDDHandler"><a href="#2-3-2-1-创建KafkaRDD处理类：KafkaRDDHandler" class="headerlink" title="2.3.2.1 创建KafkaRDD处理类：KafkaRDDHandler"></a>2.3.2.1 创建KafkaRDD处理类：KafkaRDDHandler</h4><p>该类主要是用来处理Spark读取Kafka数据之后生成的RDD，把我们读到静态数据加载到这个RDD中，重新返回新的RDD，因为涉及到数据的加载，所以这个地方还要根据我们传入的参数动态生成schema，也就是Kafka数据与静态数据合并之后的schema。</p>
<h5 id="schema的生成："><a href="#schema的生成：" class="headerlink" title="schema的生成："></a>schema的生成：</h5><p>原本spark读取kafka数据生成的schema是如下格式的：</p>
<pre><code class="scala">// 源码位置：org.apache.spark.sql.kafka010.KafkaOffsetReader
def kafkaSchema: StructType = StructType(Seq(
  StructField(&quot;key&quot;, BinaryType),
  StructField(&quot;value&quot;, BinaryType),
  StructField(&quot;topic&quot;, StringType),
  StructField(&quot;partition&quot;, IntegerType),
  StructField(&quot;offset&quot;, LongType),
  StructField(&quot;timestamp&quot;, TimestampType),
  StructField(&quot;timestampType&quot;, IntegerType)
))
</code></pre>
<p>既然我们要将参数数据写进DataFrame，相应的就需要修改schema，这里我们动态重写value列，将schema拆成两部分，一个是kafka基本信息部分(除了value列，所有的列：key、topic、partition、offset、timestamp、timestampType)，另一个是value列，这里value列就不是BinaryType类型了，而是一个嵌套类型。嵌套类型的schema是根据kafka里的json数据以及rest请求到的json数据动态生成的。</p>
<pre><code class="scala">val kafkaSchema: List[StructField] = List[StructField](
  StructField(&quot;key&quot;, BinaryType),
  StructField(&quot;topic&quot;, StringType),
  StructField(&quot;partition&quot;, IntegerType),
  StructField(&quot;offset&quot;, LongType),
  StructField(&quot;timestamp&quot;, TimestampType),
  StructField(&quot;timestampType&quot;, IntegerType)
)
val valueSchema: List[StructField] = List[StructField](
  StructField(&quot;namespace&quot;, StringType),
  StructField(&quot;v&quot;, DoubleType),
  StructField(&quot;s&quot;, IntegerType),
  StructField(&quot;t&quot;, TimestampType),
  StructField(&quot;gatewayId&quot;, StringType),
  StructField(&quot;pointId&quot;, StringType)
)
val DC_CLEANING_RULE_PREFIX = &quot;dc.cleaning.rule.&quot;
val DC_CLEANING_RULE_COLUMN = &quot;column.&quot;
val SCHEMA_VALUE_NAME = &quot;value&quot;

def getDCParameters(parameters: Map[String, String]): Map[String, String] = parameters
  .filter(_._1.startsWith(DC_CLEANING_RULE_PREFIX))
  .map(parameter =&gt; (parameter._1.stripPrefix(DC_CLEANING_RULE_PREFIX), parameter._2))

def getColumnParameters(dcParameters: Map[String, String]): Map[String, String] = dcParameters
  .filter(_._1.contains(DC_CLEANING_RULE_COLUMN))
  .map(parameter =&gt; (parameter._1.stripPrefix(DC_CLEANING_RULE_COLUMN), parameter._2))

def schema(parameters: Map[String, String]): StructType = &#123;
  val dcParams = getDCParameters(parameters)
  generateSchema(getColumnParameters(dcParams))
&#125;

def generateSchema(columnParameters: Map[String, String]): StructType = &#123;
  val newValueSchema = valueSchema ::: columnParameters.map(p =&gt; StructField(p._1, StringType)).toList
  StructType(kafkaSchema :+ StructField(SCHEMA_VALUE_NAME, StructType(newValueSchema)))
&#125;
</code></pre>
<p>如上代码所示最终schema是由kafkaSchema和valueSchema嵌套生成的，测试如下：</p>
<pre><code class="scala">val params = Map[String,String](&quot;dc.cleaning.rule.column.test&quot;-&gt;&quot;test&quot;)
schema(params).printTreeString()
/*
    root
 |-- key: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
 |-- value: struct (nullable = true)
 |    |-- namespace: string (nullable = true)
 |    |-- v: double (nullable = true)
 |    |-- s: integer (nullable = true)
 |    |-- t: timestamp (nullable = true)
 |    |-- gatewayId: string (nullable = true)
 |    |-- pointId: string (nullable = true)
 |    |-- test: string (nullable = true)
 */
</code></pre>
<h5 id="REST获取数据"><a href="#REST获取数据" class="headerlink" title="REST获取数据"></a>REST获取数据</h5><p>通过http请求获取数据，并将数据展开为k,v格式，如下所示：</p>
<pre><code class="scala"> /**
    * 请求规则
    *
    * @param api   请求地址
    * @param retry 重试次数
    * @return Map[pointId,instanceParams]
    */
  def requestRules(api: String, retry: Int = 10): Map[String, String] = &#123;
    var retries = 0
    var rules = Map[String, String]()
    import scala.collection.JavaConverters._
    while (retries &lt; retry) &#123;
      // Request data
      val response = Request.Get(api).execute().returnResponse()
      if (response.getStatusLine.getStatusCode == 200) &#123;
        try &#123;
          val res = JSON.parseObject(response.getEntity.getContent, classOf[JSONObject]).asInstanceOf[JSONObject]

          /**
            * 解析json数据，并展开成Map格式，key为pointId,value为instanceParams
            * 如：
            * [&#123;
            * &quot;instanceId&quot;: &quot;1&quot;,
            * &quot;instanceParams&quot;: &quot;[&#123;\&quot;label\&quot;:\&quot;上限值\&quot;,\&quot;name\&quot;:\&quot;upperbound\&quot;,\&quot;value\&quot;:\&quot;100.823\&quot;&#125;,
            * &#123;\&quot;label\&quot;:\&quot;下限值\&quot;,\&quot;name\&quot;:\&quot;lowerbound\&quot;,\&quot;value\&quot;:\&quot;50.534\&quot;&#125;]&quot;,
            * &quot;pointId&quot;: [
            * &quot;hiacloud/000000F&quot;,
            * &quot;hiacloud/000002F&quot;,
            * &quot;hiacloud/000001F&quot;
            * ]
            * &#125;]
            * 转成如下格式：
            * (&quot;hiacloud/000000F&quot;-&gt;&quot;[&#123;\&quot;label\&quot;:\&quot;上限值\&quot;,\&quot;name\&quot;:\&quot;upperbound\&quot;,\&quot;value\&quot;:\&quot;100.823\&quot;&#125;,
            * &#123;\&quot;label\&quot;:\&quot;下限值\&quot;,\&quot;name\&quot;:\&quot;lowerbound\&quot;,\&quot;value\&quot;:\&quot;50.534\&quot;&#125;]&quot;)
            * ……
            */
          rules = res.getJSONArray(&quot;data&quot;).asScala.flatMap(x =&gt; &#123;
            val j = x.asInstanceOf[JSONObject]
            val instanceParams = j.getString(&quot;instanceParams&quot;)
            val pointIds = j.getJSONArray(&quot;pointId&quot;)
            pointIds.asScala.toArray.map(x =&gt; x.toString -&gt; instanceParams)
          &#125;).toMap
        &#125; catch &#123;
          case e: Exception =&gt; retries += 1
        &#125;
        retries = retry
      &#125;
      else retries += 1
    &#125;
    rules
  &#125;
</code></pre>
<p>测试如下：</p>
<pre><code class="scala">    val api = &quot;http://192.168.66.194:8088/rulemgr/ruleInstanceWithBindByCode?code=shxxpb&quot;
    println(requestRules(api))
    /*
    Map(
        hiacloud/000002F -&gt; [&#123;&quot;label&quot;:&quot;上限值&quot;,&quot;name&quot;:&quot;upperbound&quot;,&quot;value&quot;:&quot;100.823&quot;&#125;,&#123;&quot;label&quot;:&quot;下限值&quot;,&quot;name&quot;:&quot;lowerbound&quot;,&quot;value&quot;:&quot;50.534&quot;&#125;],
        hiacloud/000001F -&gt; [&#123;&quot;label&quot;:&quot;上限值&quot;,&quot;name&quot;:&quot;upperbound&quot;,&quot;value&quot;:&quot;100.823&quot;&#125;,&#123;&quot;label&quot;:&quot;下限值&quot;,&quot;name&quot;:&quot;lowerbound&quot;,&quot;value&quot;:&quot;50.534&quot;&#125;], 
    */
</code></pre>
<h5 id="处理kafka数据，与REST数据合并"><a href="#处理kafka数据，与REST数据合并" class="headerlink" title="处理kafka数据，与REST数据合并"></a>处理kafka数据，与REST数据合并</h5><pre><code class="scala">def handleMessage(values: Array[Byte]): InternalRow = &#123;
  val g = new Gson()
  // 处理kafka数据，转为json格式
  val message = g.fromJson(Bytes.toString(values), classOf[Array[Message]])(0)
  var row = InternalRow()
  try &#123;
    // 生成基本数据
    val rowList: List[Any] = List[Any](
      UTF8String.fromString(message.namespace),
      message.v.substring(2).toDouble,
      message.s,
      DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(message.t)),
      UTF8String.fromString(message.gatewayId),
      UTF8String.fromString(message.pointId)
    )
    val ruleKey = message.gatewayId + &quot;/&quot; + message.pointId
    var newRowList: List[Any] = List[Any]()
    // 解析REST数据
    if (rules.contains(ruleKey)) &#123;
      val rule = rules(ruleKey)
      newRowList = columnParameters.map(x =&gt; UTF8String.fromString(JSONPath.read(rule, x._2).asInstanceOf[String])).toList
    &#125; else &#123;
      newRowList = columnParameters.map(x =&gt; UTF8String.EMPTY_UTF8).toList
    &#125;
    // 合并数据
    row = InternalRow.fromSeq(rowList ::: newRowList)

  &#125; catch &#123;
    case e: Exception =&gt; println(&quot;&quot;)
  &#125;
  row
&#125;
</code></pre>
<h5 id="数据写入RDD"><a href="#数据写入RDD" class="headerlink" title="数据写入RDD"></a>数据写入RDD</h5><pre><code class="scala">  def handle(rdd: KafkaSourceRDD): RDD[InternalRow] = &#123;
    reloadRules()
    rdd.map(cr =&gt; &#123;
      val ms = handleMessage(cr.value())
      InternalRow(
        cr.key,
        UTF8String.fromString(cr.topic),
        cr.partition,
        cr.offset,
        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),
        cr.timestampType.id,
        ms
      )
    &#125;)
  &#125;
</code></pre>
<h5 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h5><pre><code class="scala">package org.apache.spark.sql.kafka010

import com.alibaba.fastjson.&#123;JSON, JSONObject, JSONPath&#125;
import com.google.gson.Gson
import org.apache.hadoop.hbase.util.Bytes
import org.apache.http.client.fluent.Request
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.util.DateTimeUtils
import org.apache.spark.sql.types._
import org.apache.spark.unsafe.types.UTF8String


/**
  * @author : shirukai
  * @date : 2019-01-22 11:03
  *       KafkaSourceRDD 处理器
  */


private[kafka010] class KafkaSourceRDDHandler(parameters: Map[String, String], batch: Long = 100) extends Serializable &#123;

  import KafkaSourceRDDHandler._

  val dcParameters: Map[String, String] = getDCParameters(parameters)

  val columnParameters: Map[String, String] = getColumnParameters(dcParameters)

  var offset = 0

  val batchSize: Long = dcParameters.getOrElse(DC_CLEANING_RULE_REQUEST_BATCH, batch).toString.toLong

  val api: String = dcParameters.getOrElse(DC_CLEANING_RULE_SERVICE_API, DC_CLEANING_RULE_SERVICE_API_DEFAULT)

  lazy val schema: StructType = generateSchema(columnParameters)

  var rules: Map[String, String] = Map[String, String]()

  def handle(rdd: KafkaSourceRDD): RDD[InternalRow] = &#123;
    reloadRules()

    rdd.map(cr =&gt; &#123;
      val ms = handleMessage(cr.value())
      InternalRow(
        cr.key,
        UTF8String.fromString(cr.topic),
        cr.partition,
        cr.offset,
        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),
        cr.timestampType.id,
        ms
      )
    &#125;)
  &#125;

  def reloadRules(): Unit = &#123;
    if (offset == batchSize || offset == 0) &#123;
      offset = 0
      rules = requestRules(api)
    &#125;
    offset += 1
  &#125;

  def handleMessage(values: Array[Byte]): InternalRow = &#123;
    val g = new Gson()
    // 处理kafka数据，转为json格式
    val message = g.fromJson(Bytes.toString(values), classOf[Array[Message]])(0)
    var row = InternalRow()
    try &#123;
      // 生成基本数据
      val rowList: List[Any] = List[Any](
        UTF8String.fromString(message.namespace),
        message.v.substring(2).toDouble,
        message.s,
        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(message.t)),
        UTF8String.fromString(message.gatewayId),
        UTF8String.fromString(message.pointId)
      )
      val ruleKey = message.gatewayId + &quot;/&quot; + message.pointId
      var newRowList: List[Any] = List[Any]()
      // 解析REST数据
      if (rules.contains(ruleKey)) &#123;
        val rule = rules(ruleKey)
        newRowList = columnParameters.map(x =&gt; UTF8String.fromString(JSONPath.read(rule, x._2).asInstanceOf[String])).toList
      &#125; else &#123;
        newRowList = columnParameters.map(x =&gt; UTF8String.EMPTY_UTF8).toList
      &#125;
      // 合并数据
      row = InternalRow.fromSeq(rowList ::: newRowList)

    &#125; catch &#123;
      case e: Exception =&gt; println(&quot;&quot;)
    &#125;
    row
  &#125;

&#125;

private[kafka010] object KafkaSourceRDDHandler extends Serializable &#123;
  val kafkaSchema: List[StructField] = List[StructField](
    StructField(&quot;key&quot;, BinaryType),
    StructField(&quot;topic&quot;, StringType),
    StructField(&quot;partition&quot;, IntegerType),
    StructField(&quot;offset&quot;, LongType),
    StructField(&quot;timestamp&quot;, TimestampType),
    StructField(&quot;timestampType&quot;, IntegerType)
  )
  val valueSchema: List[StructField] = List[StructField](
    StructField(&quot;namespace&quot;, StringType),
    StructField(&quot;v&quot;, DoubleType),
    StructField(&quot;s&quot;, IntegerType),
    StructField(&quot;t&quot;, TimestampType),
    StructField(&quot;gatewayId&quot;, StringType),
    StructField(&quot;pointId&quot;, StringType)
  )
  val DC_CLEANING_RULE_PREFIX = &quot;dc.cleaning.rule.&quot;
  val DC_CLEANING_RULE_REQUEST_BATCH = &quot;dc.cleaning.rule.request.batch&quot;
  val DC_CLEANING_RULE_COLUMN = &quot;column.&quot;
  val DC_CLEANING_RULE_SERVICE_API = &quot;service.api&quot;
  val DC_CLEANING_RULE_SERVICE_API_DEFAULT = &quot;http://192.168.66.194:8088/rulemgr/ruleInstanceWithBindByCode?code=shxxpb&quot;
  val SCHEMA_VALUE_NAME = &quot;value&quot;

  def getDCParameters(parameters: Map[String, String]): Map[String, String] = parameters
    .filter(_._1.startsWith(DC_CLEANING_RULE_PREFIX))
    .map(parameter =&gt; (parameter._1.stripPrefix(DC_CLEANING_RULE_PREFIX), parameter._2))

  def getColumnParameters(dcParameters: Map[String, String]): Map[String, String] = dcParameters
    .filter(_._1.contains(DC_CLEANING_RULE_COLUMN))
    .map(parameter =&gt; (parameter._1.stripPrefix(DC_CLEANING_RULE_COLUMN), parameter._2))

  def schema(parameters: Map[String, String]): StructType = &#123;
    val dcParams = getDCParameters(parameters)
    generateSchema(getColumnParameters(dcParams))
  &#125;

  def generateSchema(columnParameters: Map[String, String]): StructType = &#123;
    val newValueSchema = valueSchema ::: columnParameters.map(p =&gt; StructField(p._1, DataTypes.StringType)).toList
    StructType(kafkaSchema :+ StructField(SCHEMA_VALUE_NAME, StructType(newValueSchema)))
  &#125;

  /**
    * 请求规则
    *
    * @param api   请求地址
    * @param retry 重试次数
    * @return Map[pointId,instanceParams]
    */
  def requestRules(api: String, retry: Int = 10): Map[String, String] = &#123;
    var retries = 0
    var rules = Map[String, String]()
    import scala.collection.JavaConverters._
    while (retries &lt; retry) &#123;
      // Request data
      val response = Request.Get(api).execute().returnResponse()
      if (response.getStatusLine.getStatusCode == 200) &#123;
        try &#123;
          val res = JSON.parseObject(response.getEntity.getContent, classOf[JSONObject]).asInstanceOf[JSONObject]

          /**
            * 解析json数据，并展开成Map格式，key为pointId,value为instanceParams
            * 如：
            * [&#123;
            * &quot;instanceId&quot;: &quot;1&quot;,
            * &quot;instanceParams&quot;: &quot;[&#123;\&quot;label\&quot;:\&quot;上限值\&quot;,\&quot;name\&quot;:\&quot;upperbound\&quot;,\&quot;value\&quot;:\&quot;100.823\&quot;&#125;,
            * &#123;\&quot;label\&quot;:\&quot;下限值\&quot;,\&quot;name\&quot;:\&quot;lowerbound\&quot;,\&quot;value\&quot;:\&quot;50.534\&quot;&#125;]&quot;,
            * &quot;pointId&quot;: [
            * &quot;hiacloud/000000F&quot;,
            * &quot;hiacloud/000002F&quot;,
            * &quot;hiacloud/000001F&quot;
            * ]
            * &#125;]
            * 转成如下格式：
            * (&quot;hiacloud/000000F&quot;-&gt;&quot;[&#123;\&quot;label\&quot;:\&quot;上限值\&quot;,\&quot;name\&quot;:\&quot;upperbound\&quot;,\&quot;value\&quot;:\&quot;100.823\&quot;&#125;,
            * &#123;\&quot;label\&quot;:\&quot;下限值\&quot;,\&quot;name\&quot;:\&quot;lowerbound\&quot;,\&quot;value\&quot;:\&quot;50.534\&quot;&#125;]&quot;)
            * ……
            */
          rules = res.getJSONArray(&quot;data&quot;).asScala.flatMap(x =&gt; &#123;
            val j = x.asInstanceOf[JSONObject]
            val instanceParams = j.getString(&quot;instanceParams&quot;)
            val pointIds = j.getJSONArray(&quot;pointId&quot;)
            pointIds.asScala.toArray.map(x =&gt; x.toString -&gt; instanceParams)
          &#125;).toMap
        &#125; catch &#123;
          case e: Exception =&gt; retries += 1
        &#125;
        retries = retry
      &#125;
      else retries += 1
    &#125;
    rules
  &#125;


  case class Message(namespace: String, v: String, s: Int, t: Long, gatewayId: String, pointId: String)

  def main(args: Array[String]): Unit = &#123;
    val params = Map[String,String](&quot;dc.cleaning.rule.column.test&quot;-&gt;&quot;test&quot;)
    schema(params).printTreeString()
    /*
        root
     |-- key: binary (nullable = true)
     |-- topic: string (nullable = true)
     |-- partition: integer (nullable = true)
     |-- offset: long (nullable = true)
     |-- timestamp: timestamp (nullable = true)
     |-- timestampType: integer (nullable = true)
     |-- value: struct (nullable = true)
     |    |-- namespace: string (nullable = true)
     |    |-- v: double (nullable = true)
     |    |-- s: integer (nullable = true)
     |    |-- t: timestamp (nullable = true)
     |    |-- gatewayId: string (nullable = true)
     |    |-- pointId: string (nullable = true)
     |    |-- test: string (nullable = true)
     */

    val api = &quot;http://192.168.66.194:8088/rulemgr/ruleInstanceWithBindByCode?code=shxxpb&quot;
    println(requestRules(api))
    /*
    Map(
        hiacloud/000002F -&gt; [&#123;&quot;label&quot;:&quot;上限值&quot;,&quot;name&quot;:&quot;upperbound&quot;,&quot;value&quot;:&quot;100.823&quot;&#125;,&#123;&quot;label&quot;:&quot;下限值&quot;,&quot;name&quot;:&quot;lowerbound&quot;,&quot;value&quot;:&quot;50.534&quot;&#125;],
        hiacloud/000001F -&gt; [&#123;&quot;label&quot;:&quot;上限值&quot;,&quot;name&quot;:&quot;upperbound&quot;,&quot;value&quot;:&quot;100.823&quot;&#125;,&#123;&quot;label&quot;:&quot;下限值&quot;,&quot;name&quot;:&quot;lowerbound&quot;,&quot;value&quot;:&quot;50.534&quot;&#125;],
    */
  &#125;

&#125;
</code></pre>
<h4 id="2-3-2-2-重写Source类：KafkaWithRuleSource"><a href="#2-3-2-2-重写Source类：KafkaWithRuleSource" class="headerlink" title="2.3.2.2 重写Source类：KafkaWithRuleSource"></a>2.3.2.2 重写Source类：KafkaWithRuleSource</h4><p>上一步，我们的重点工作已经完成，下面要将我们写的KafkaSourceRDDHandler应用到KafkaSource中，所以我们要重写Source类，可以直接将KafkaSource的代码直接拷贝过来，修改部分代码即可。KafkaSource位置:org.apache.spark.sql.kafka010.KafkaSource。主要修改内容如下所示：</p>
<h5 id="实例化KafkaSourceRDDHandler"><a href="#实例化KafkaSourceRDDHandler" class="headerlink" title="实例化KafkaSourceRDDHandler"></a>实例化KafkaSourceRDDHandler</h5><pre><code class="scala">private val ksrh = new KafkaSourceRDDHandler(sourceOptions)
</code></pre>
<h5 id="重写Schema"><a href="#重写Schema" class="headerlink" title="重写Schema"></a>重写Schema</h5><pre><code class="scala">override def schema: StructType = ksrh.schema
</code></pre>
<h5 id="处理KafkaRDD"><a href="#处理KafkaRDD" class="headerlink" title="处理KafkaRDD"></a>处理KafkaRDD</h5><p>注释原来的创建RDD的代码，使用我们上一步写的KafkaSourceRDDHandle来对RDD进行处理</p>
<pre><code class="scala">// Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.
//    val rdd = new KafkaSourceRDD(
//      sc, executorKafkaParams, offsetRanges, pollTimeoutMs, failOnDataLoss,
//      reuseKafkaConsumer = true).map &#123; cr =&gt;
//      InternalRow(
//        cr.key,
//        cr.value,
//        UTF8String.fromString(cr.topic),
//        cr.partition,
//        cr.offset,
//        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),
//        cr.timestampType.id,
//        UTF8String.fromBytes(cr.value())
//      )
//    &#125;
val rdd = ksrh.handle(new KafkaSourceRDD(
  sc, executorKafkaParams, offsetRanges, pollTimeoutMs, failOnDataLoss,
  reuseKafkaConsumer = true
))
</code></pre>
<h5 id="完整代码-1"><a href="#完整代码-1" class="headerlink" title="完整代码"></a>完整代码</h5><pre><code class="scala">/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the &quot;License&quot;); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.kafka010

import java.&#123;util =&gt; ju&#125;
import java.io._
import java.nio.charset.StandardCharsets

import org.apache.commons.io.IOUtils
import org.apache.kafka.common.TopicPartition
import org.apache.spark.SparkContext
import org.apache.spark.internal.Logging
import org.apache.spark.scheduler.ExecutorCacheTaskLocation
import org.apache.spark.sql._
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.util.DateTimeUtils
import org.apache.spark.sql.execution.streaming._
import org.apache.spark.sql.kafka010.KafkaSource._
import org.apache.spark.sql.types.&#123;StructField, _&#125;
import org.apache.spark.unsafe.types.UTF8String

/**
  * A [[Source]] that reads data from Kafka using the following design.
  *
  * - The [[KafkaSourceOffset]] is the custom [[Offset]] defined for this source that contains
  * a map of TopicPartition -&gt; offset. Note that this offset is 1 + (available offset). For
  * example if the last record in a Kafka topic &quot;t&quot;, partition 2 is offset 5, then
  * KafkaSourceOffset will contain TopicPartition(&quot;t&quot;, 2) -&gt; 6. This is done keep it consistent
  * with the semantics of `KafkaConsumer.position()`.
  *
  * - The [[KafkaSource]] written to do the following.
  *
  *  - As soon as the source is created, the pre-configured [[KafkaOffsetReader]]
  * is used to query the initial offsets that this source should
  * start reading from. This is used to create the first batch.
  *
  *   - `getOffset()` uses the [[KafkaOffsetReader]] to query the latest
  * available offsets, which are returned as a [[KafkaSourceOffset]].
  *
  *   - `getBatch()` returns a DF that reads from the &#39;start offset&#39; until the &#39;end offset&#39; in
  * for each partition. The end offset is excluded to be consistent with the semantics of
  * [[KafkaSourceOffset]] and `KafkaConsumer.position()`.
  *
  *   - The DF returned is based on [[KafkaSourceRDD]] which is constructed such that the
  * data from Kafka topic + partition is consistently read by the same executors across
  * batches, and cached KafkaConsumers in the executors can be reused efficiently. See the
  * docs on [[KafkaSourceRDD]] for more details.
  *
  * Zero data lost is not guaranteed when topics are deleted. If zero data lost is critical, the user
  * must make sure all messages in a topic have been processed when deleting a topic.
  *
  * There is a known issue caused by KAFKA-1894: the query using KafkaSource maybe cannot be stopped.
  * To avoid this issue, you should make sure stopping the query before stopping the Kafka brokers
  * and not use wrong broker addresses.
  */
private[kafka010] class KafkaWithRuleSource(
                                             sqlContext: SQLContext,
                                             kafkaReader: KafkaOffsetReader,
                                             executorKafkaParams: ju.Map[String, Object],
                                             sourceOptions: Map[String, String],
                                             metadataPath: String,
                                             startingOffsets: KafkaOffsetRangeLimit,
                                             failOnDataLoss: Boolean)
  extends Source with Logging &#123;

  private val ksrh = new KafkaSourceRDDHandler(sourceOptions)
  private val sc = sqlContext.sparkContext

  private val pollTimeoutMs = sourceOptions.getOrElse(
    &quot;kafkaConsumer.pollTimeoutMs&quot;,
    sc.conf.getTimeAsMs(&quot;spark.network.timeout&quot;, &quot;120s&quot;).toString
  ).toLong

  private val maxOffsetsPerTrigger =
    sourceOptions.get(&quot;maxOffsetsPerTrigger&quot;).map(_.toLong)

  /**
    * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only
    * called in StreamExecutionThread. Otherwise, interrupting a thread while running
    * `KafkaConsumer.poll` may hang forever (KAFKA-1894).
    */
  private lazy val initialPartitionOffsets = &#123;
    val metadataLog =
      new HDFSMetadataLog[KafkaSourceOffset](sqlContext.sparkSession, metadataPath) &#123;
        override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = &#123;
          out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)
          val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))
          writer.write(&quot;v&quot; + VERSION + &quot;\n&quot;)
          writer.write(metadata.json)
          writer.flush
        &#125;

        override def deserialize(in: InputStream): KafkaSourceOffset = &#123;
          in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)
          val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))
          // HDFSMetadataLog guarantees that it never creates a partial file.
          assert(content.length != 0)
          if (content(0) == &#39;v&#39;) &#123;
            val indexOfNewLine = content.indexOf(&quot;\n&quot;)
            if (indexOfNewLine &gt; 0) &#123;
              val version = parseVersion(content.substring(0, indexOfNewLine), VERSION)
              KafkaSourceOffset(SerializedOffset(content.substring(indexOfNewLine + 1)))
            &#125; else &#123;
              throw new IllegalStateException(
                s&quot;Log file was malformed: failed to detect the log file version line.&quot;)
            &#125;
          &#125; else &#123;
            // The log was generated by Spark 2.1.0
            KafkaSourceOffset(SerializedOffset(content))
          &#125;
        &#125;
      &#125;

    metadataLog.get(0).getOrElse &#123;
      val offsets = startingOffsets match &#123;
        case EarliestOffsetRangeLimit =&gt; KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())
        case LatestOffsetRangeLimit =&gt; KafkaSourceOffset(kafkaReader.fetchLatestOffsets())
        case SpecificOffsetRangeLimit(p) =&gt; kafkaReader.fetchSpecificOffsets(p, reportDataLoss)
      &#125;
      metadataLog.add(0, offsets)
      logInfo(s&quot;Initial offsets: $offsets&quot;)
      offsets
    &#125;.partitionToOffsets
  &#125;

  private var currentPartitionOffsets: Option[Map[TopicPartition, Long]] = None

  override def schema: StructType = ksrh.schema

  /** Returns the maximum available offset for this source.
    * 获取偏移量
    * */
  override def getOffset: Option[Offset] = &#123;
    // Make sure initialPartitionOffsets is initialized
    initialPartitionOffsets

    // 获取最新的偏移量
    val latest = kafkaReader.fetchLatestOffsets()

    // 看看有没有指定每个Trigger最大的偏移量
    val offsets = maxOffsetsPerTrigger match &#123;
      case None =&gt;
        // 如果没有指定，返回最新的偏移量
        latest
      case Some(limit) if currentPartitionOffsets.isEmpty =&gt;
        // 如果指定了，并判断当前分区的偏移量是否为空
        // 如果为空，说明之前没有消费过，取初始化的offset的
        // 初始化的offset是从元数据里取的（hdfs或这文件）
        rateLimit(limit, initialPartitionOffsets, latest)
      case Some(limit) =&gt;
        // 如果当前偏移量不为空，根据当前的偏移量还有最新的偏移量以及每个trigger的最大偏移量得到这一批次的偏移量
        rateLimit(limit, currentPartitionOffsets.get, latest)
    &#125;

    currentPartitionOffsets = Some(offsets)
    logDebug(s&quot;GetOffset: $&#123;offsets.toSeq.map(_.toString).sorted&#125;&quot;)
    Some(KafkaSourceOffset(offsets))
  &#125;

  /** Proportionally distribute limit number of offsets among topicpartitions */
  private def rateLimit(
                         limit: Long,
                         from: Map[TopicPartition, Long],
                         until: Map[TopicPartition, Long]): Map[TopicPartition, Long] = &#123;
    val fromNew = kafkaReader.fetchEarliestOffsets(until.keySet.diff(from.keySet).toSeq)
    val sizes = until.flatMap &#123;
      case (tp, end) =&gt;
        // If begin isn&#39;t defined, something&#39;s wrong, but let alert logic in getBatch handle it
        from.get(tp).orElse(fromNew.get(tp)).flatMap &#123; begin =&gt;
          val size = end - begin
          logDebug(s&quot;rateLimit $tp size is $size&quot;)
          if (size &gt; 0) Some(tp -&gt; size) else None
        &#125;
    &#125;
    val total = sizes.values.sum.toDouble
    if (total &lt; 1) &#123;
      until
    &#125; else &#123;
      until.map &#123;
        case (tp, end) =&gt;
          tp -&gt; sizes.get(tp).map &#123; size =&gt;
            val begin = from.get(tp).getOrElse(fromNew(tp))
            val prorate = limit * (size / total)
            logDebug(s&quot;rateLimit $tp prorated amount is $prorate&quot;)
            // Don&#39;t completely starve small topicpartitions
            val off = begin + (if (prorate &lt; 1) Math.ceil(prorate) else Math.floor(prorate)).toLong
            logDebug(s&quot;rateLimit $tp new offset is $off&quot;)
            // Paranoia, make sure not to return an offset that&#39;s past end
            Math.min(end, off)
          &#125;.getOrElse(end)
      &#125;
    &#125;
  &#125;

  /**
    * Returns the data that is between the offsets
    * [`start.get.partitionToOffsets`, `end.partitionToOffsets`), i.e. end.partitionToOffsets is
    * exclusive.
    */
  override def getBatch(start: Option[Offset], end: Offset): DataFrame = &#123;
    // Make sure initialPartitionOffsets is initialized
    initialPartitionOffsets

    logInfo(s&quot;GetBatch called with start = $start, end = $end&quot;)
    val untilPartitionOffsets = KafkaSourceOffset.getPartitionOffsets(end)
    // On recovery, getBatch will get called before getOffset
    if (currentPartitionOffsets.isEmpty) &#123;
      currentPartitionOffsets = Some(untilPartitionOffsets)
    &#125;
    if (start.isDefined &amp;&amp; start.get == end) &#123;
      return sqlContext.internalCreateDataFrame(
        sqlContext.sparkContext.emptyRDD, schema, isStreaming = true)
    &#125;
    val fromPartitionOffsets = start match &#123;
      case Some(prevBatchEndOffset) =&gt;
        KafkaSourceOffset.getPartitionOffsets(prevBatchEndOffset)
      case None =&gt;
        initialPartitionOffsets
    &#125;

    // Find the new partitions, and get their earliest offsets
    val newPartitions = untilPartitionOffsets.keySet.diff(fromPartitionOffsets.keySet)
    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)
    if (newPartitionOffsets.keySet != newPartitions) &#123;
      // We cannot get from offsets for some partitions. It means they got deleted.
      val deletedPartitions = newPartitions.diff(newPartitionOffsets.keySet)
      reportDataLoss(
        s&quot;Cannot find earliest offsets of $&#123;deletedPartitions&#125;. Some data may have been missed&quot;)
    &#125;
    logInfo(s&quot;Partitions added: $newPartitionOffsets&quot;)
    newPartitionOffsets.filter(_._2 != 0).foreach &#123; case (p, o) =&gt;
      reportDataLoss(
        s&quot;Added partition $p starts from $o instead of 0. Some data may have been missed&quot;)
    &#125;

    val deletedPartitions = fromPartitionOffsets.keySet.diff(untilPartitionOffsets.keySet)
    if (deletedPartitions.nonEmpty) &#123;
      reportDataLoss(s&quot;$deletedPartitions are gone. Some data may have been missed&quot;)
    &#125;

    // Use the until partitions to calculate offset ranges to ignore partitions that have
    // been deleted
    val topicPartitions = untilPartitionOffsets.keySet.filter &#123; tp =&gt;
      // Ignore partitions that we don&#39;t know the from offsets.
      newPartitionOffsets.contains(tp) || fromPartitionOffsets.contains(tp)
    &#125;.toSeq
    logDebug(&quot;TopicPartitions: &quot; + topicPartitions.mkString(&quot;, &quot;))

    val sortedExecutors = getSortedExecutorList(sc)
    val numExecutors = sortedExecutors.length
    logDebug(&quot;Sorted executors: &quot; + sortedExecutors.mkString(&quot;, &quot;))

    // Calculate offset ranges
    val offsetRanges = topicPartitions.map &#123; tp =&gt;
      val fromOffset = fromPartitionOffsets.get(tp).getOrElse &#123;
        newPartitionOffsets.getOrElse(tp, &#123;
          // This should not happen since newPartitionOffsets contains all partitions not in
          // fromPartitionOffsets
          throw new IllegalStateException(s&quot;$tp doesn&#39;t have a from offset&quot;)
        &#125;)
      &#125;
      val untilOffset = untilPartitionOffsets(tp)
      val preferredLoc = if (numExecutors &gt; 0) &#123;
        // This allows cached KafkaConsumers in the executors to be re-used to read the same
        // partition in every batch.
        Some(sortedExecutors(Math.floorMod(tp.hashCode, numExecutors)))
      &#125; else None
      KafkaSourceRDDOffsetRange(tp, fromOffset, untilOffset, preferredLoc)
    &#125;.filter &#123; range =&gt;
      if (range.untilOffset &lt; range.fromOffset) &#123;
        reportDataLoss(s&quot;Partition $&#123;range.topicPartition&#125;&#39;s offset was changed from &quot; +
          s&quot;$&#123;range.fromOffset&#125; to $&#123;range.untilOffset&#125;, some data may have been missed&quot;)
        false
      &#125; else &#123;
        true
      &#125;
    &#125;.toArray

    // Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.
    //    val rdd = new KafkaSourceRDD(
    //      sc, executorKafkaParams, offsetRanges, pollTimeoutMs, failOnDataLoss,
    //      reuseKafkaConsumer = true).map &#123; cr =&gt;
    //      InternalRow(
    //        cr.key,
    //        cr.value,
    //        UTF8String.fromString(cr.topic),
    //        cr.partition,
    //        cr.offset,
    //        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),
    //        cr.timestampType.id,
    //        UTF8String.fromBytes(cr.value())
    //      )
    //    &#125;
    val rdd = ksrh.handle(new KafkaSourceRDD(
      sc, executorKafkaParams, offsetRanges, pollTimeoutMs, failOnDataLoss,
      reuseKafkaConsumer = true
    ))


    logInfo(&quot;GetBatch generating RDD of offset range: &quot; +
      offsetRanges.sortBy(_.topicPartition.toString).mkString(&quot;, &quot;))
    sqlContext.internalCreateDataFrame(rdd, schema, isStreaming = true)
  &#125;

  /** Stop this source and free any resources it has allocated. */
  override def stop(): Unit = synchronized &#123;
    kafkaReader.close()
  &#125;

  override def toString(): String = s&quot;KafkaSource[$kafkaReader]&quot;

  /**
    * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.
    * Otherwise, just log a warning.
    */
  private def reportDataLoss(message: String): Unit = &#123;
    if (failOnDataLoss) &#123;
      throw new IllegalStateException(message + s&quot;. $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE&quot;)
    &#125; else &#123;
      logWarning(message + s&quot;. $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE&quot;)
    &#125;
  &#125;
&#125;

/** Companion object for the [[KafkaSource]]. */
private[kafka010] object KafkaWithRuleSource &#123;
  val INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE =
    &quot;&quot;&quot;
      |Some data may have been lost because they are not available in Kafka any more; either the
      | data was aged out by Kafka or the topic may have been deleted before all the data in the
      | topic was processed. If you want your streaming query to fail on such cases, set the source
      | option &quot;failOnDataLoss&quot; to &quot;true&quot;.
    &quot;&quot;&quot;.stripMargin

  val INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE =
    &quot;&quot;&quot;
      |Some data may have been lost because they are not available in Kafka any more; either the
      | data was aged out by Kafka or the topic may have been deleted before all the data in the
      | topic was processed. If you don&#39;t want your streaming query to fail on such cases, set the
      | source option &quot;failOnDataLoss&quot; to &quot;false&quot;.
    &quot;&quot;&quot;.stripMargin

  private[kafka010] val VERSION = 1

  def getSortedExecutorList(sc: SparkContext): Array[String] = &#123;
    val bm = sc.env.blockManager
    bm.master.getPeers(bm.blockManagerId).toArray
      .map(x =&gt; ExecutorCacheTaskLocation(x.host, x.executorId))
      .sortWith(compare)
      .map(_.toString)
  &#125;

  private def compare(a: ExecutorCacheTaskLocation, b: ExecutorCacheTaskLocation): Boolean = &#123;
    if (a.host == b.host) &#123;
      a.executorId &gt; b.executorId
    &#125; else &#123;
      a.host &gt; b.host
    &#125;
  &#125;

&#125;
</code></pre>
<h4 id="2-3-2-3-重写Provider类：KafkaWithRuleProvider"><a href="#2-3-2-3-重写Provider类：KafkaWithRuleProvider" class="headerlink" title="2.3.2.3 重写Provider类：KafkaWithRuleProvider"></a>2.3.2.3 重写Provider类：KafkaWithRuleProvider</h4><p>Provider类是自定义Source的入口，修改内容不大，可以直接拷贝KafkaSourceProvider，位置：org.apache.spark.sql.kafka010.KafkaSourceProvider,修改内容如下所示：</p>
<h5 id="重写sourceSchema"><a href="#重写sourceSchema" class="headerlink" title="重写sourceSchema"></a>重写sourceSchema</h5><pre><code class="scala">  override def sourceSchema(
                             sqlContext: SQLContext,
                             schema: Option[StructType],
                             providerName: String,
                             parameters: Map[String, String]): (String, StructType) = &#123;
    validateStreamOptions(parameters)
    require(schema.isEmpty, &quot;Kafka source has a fixed schema and cannot be set with a custom one&quot;)

    (shortName(), KafkaSourceRDDHandler.schema(parameters))
  &#125;
</code></pre>
<h5 id="重写createSource"><a href="#重写createSource" class="headerlink" title="重写createSource"></a>重写createSource</h5><pre><code class="scala">override def createSource(
                           sqlContext: SQLContext,
                           metadataPath: String,
                           schema: Option[StructType],
                           providerName: String,
                           parameters: Map[String, String]): Source = &#123;
  validateStreamOptions(parameters)
  // Each running query should use its own group id. Otherwise, the query may be only assigned
  // partial data since Kafka will assign partitions to multiple consumers having the same group
  // id. Hence, we should generate a unique id for each query.
  val uniqueGroupId = s&quot;spark-kafka-source-$&#123;UUID.randomUUID&#125;-$&#123;metadataPath.hashCode&#125;&quot;

  val caseInsensitiveParams = parameters.map &#123; case (k, v) =&gt; (k.toLowerCase(Locale.ROOT), v) &#125;
  val specifiedKafkaParams =
    parameters
      .keySet
      .filter(_.toLowerCase(Locale.ROOT).startsWith(&quot;kafka.&quot;))
      .map &#123; k =&gt; k.drop(6).toString -&gt; parameters(k) &#125;
      .toMap

  val startingStreamOffsets = KafkaSourceProvider.getKafkaOffsetRangeLimit(caseInsensitiveParams,
    STARTING_OFFSETS_OPTION_KEY, LatestOffsetRangeLimit)

  val kafkaOffsetReader = new KafkaOffsetReader(
    strategy(caseInsensitiveParams),
    kafkaParamsForDriver(specifiedKafkaParams),
    parameters,
    driverGroupIdPrefix = s&quot;$uniqueGroupId-driver&quot;)

  new KafkaWithRuleSource(
    sqlContext,
    kafkaOffsetReader,
    kafkaParamsForExecutors(specifiedKafkaParams, uniqueGroupId),
    parameters,
    metadataPath,
    startingStreamOffsets,
    failOnDataLoss(caseInsensitiveParams))
&#125;
</code></pre>
<h4 id="2-3-2-4-测试自定义Source"><a href="#2-3-2-4-测试自定义Source" class="headerlink" title="2.3.2.4 测试自定义Source"></a>2.3.2.4 测试自定义Source</h4><pre><code class="scala">package com.hollysys.spark.structured.datasource

import java.util.UUID

import org.apache.spark.sql.SparkSession

/**
  * @author : shirukai
  * @date : 2019-01-22 10:17
  *      参数：192.168.66.194:9092 subscribe dc-data
  */
object KafkaSourceTest &#123;
  def main(args: Array[String]): Unit = &#123;
    if (args.length &lt; 3) &#123;
      System.err.println(&quot;Usage: StructuredKafkaWordCount &lt;bootstrap-servers&gt; &quot; +
        &quot;&lt;subscribe-type&gt; &lt;topics&gt; [&lt;checkpoint-location&gt;]&quot;)
      System.exit(1)
    &#125;

    val Array(bootstrapServers, subscribeType, topics, _*) = args
    val checkpointLocation =
      if (args.length &gt; 3) args(3) else &quot;/tmp/temporary-&quot; + UUID.randomUUID.toString


    val spark = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .master(&quot;local[2]&quot;)
      .getOrCreate()
    import spark.implicits._

    // Create DataSet representing the stream of input lines from kafka
    val source = spark
      .readStream
      .format(&quot;org.apache.spark.sql.kafka010.KafkaWithRuleSourceProvider&quot;)
      .option(&quot;kafka.bootstrap.servers&quot;, bootstrapServers)
      .option(&quot;dc.cleaning.rule.column.upperbound&quot;, &quot;$.0.value&quot;)
      .option(&quot;dc.cleaning.rule.column.lowerbound&quot;, &quot;$.1.value&quot;)
      .option(&quot;dc.cleaning.rule.service.api&quot;, &quot;http://192.168.66.194:8088/rulemgr/ruleInstanceWithBindByCode?code=shxxpb&quot;)
      .option(&quot;dc.cleaning.rule.request.batch&quot;, &quot;10&quot;)
      .option(subscribeType, topics)
      .load()

    // Start running the query that prints the running counts to the console
    val query = source.select(&quot;value.*&quot;).writeStream
      .outputMode(&quot;update&quot;)
      .format(&quot;console&quot;)
      .option(&quot;checkpointLocation&quot;, checkpointLocation)
      .option(&quot;truncate&quot;, value = false).start()

    query.awaitTermination()

    /*
    Batch: 0
    -------------------------------------------
    +---------+------+---+-----------------------+---------+-------+----------+----------+
    |namespace|v     |s  |t                      |gatewayId|pointId|lowerbound|upperbound|
    +---------+------+---+-----------------------+---------+-------+----------+----------+
    |000002   |101.11|0  |2019-02-16 10:14:29.242|hiacloud |000002F|50.534    |100.823   |
    |000003   |101.11|0  |2019-02-16 10:14:29.242|hiacloud |000003F|          |          |
    |000000   |101.11|0  |2019-02-16 10:14:29.242|hiacloud |000000F|50.534    |100.823   |
    |000001   |101.11|0  |2019-02-16 10:14:29.242|hiacloud |000001F|50.534    |100.823   |
    +---------+------+---+-----------------------+---------+-------+----------+----------+
     */

  &#125;
&#125;
</code></pre>
<h1 id="3-基于重启Query的动态更新"><a href="#3-基于重启Query的动态更新" class="headerlink" title="3 基于重启Query的动态更新"></a>3 基于重启Query的动态更新</h1><h2 id="3-1-应用场景"><a href="#3-1-应用场景" class="headerlink" title="3.1 应用场景"></a>3.1 应用场景</h2><p>场景一：实时流数据与离线数据join，离线数据偶尔更新</p>
<p>场景二：某些参数偶尔需要变化，如修改写出路径、修改分区数量等等</p>
<h2 id="3-2-实现思路"><a href="#3-2-实现思路" class="headerlink" title="3.2 实现思路"></a>3.2 实现思路</h2><p>通过重启Query实现参数的动态更新。Query start之后，通过阻塞Driver端主线程，让Executor端Spark任务一直执行，一段时间后停止Query，更新参数，然后重新start。</p>
<h2 id="3-3-Demo演示"><a href="#3-3-Demo演示" class="headerlink" title="3.3 Demo演示"></a>3.3 Demo演示</h2><pre><code class="scala">package com.hollysys.spark.structured.usecase
import org.apache.spark.sql.SparkSession

/**
  * @author : shirukai
  * @date : 2019-02-16 10:43
  *       通过重启Query动态更新参数
  */
object RestQueryUpdateParams &#123;
  def main(args: Array[String]): Unit = &#123;
    val spark = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .master(&quot;local[2]&quot;)
      .getOrCreate()

    val source = spark
      .readStream
      .format(&quot;kafka&quot;)
      .option(&quot;kafka.bootstrap.servers&quot;, &quot;192.168.66.194:9092&quot;)
      .option(&quot;subscribe&quot;, &quot;dc-data&quot;)
      //.option(&quot;startingOffsets&quot;, &quot;earliest&quot;)
      .option(&quot;failOnDataLoss&quot;, &quot;true&quot;)
      .load()

    import spark.implicits._

    val restartTime = 1000 * 60

    while (true) &#123;

      // TODO 更新参数
      val query = source.writeStream
        .outputMode(&quot;update&quot;)
        .format(&quot;console&quot;)
        //.option(&quot;checkpointLocation&quot;, checkpointLocation)
        .option(&quot;truncate&quot;, value = false)
        .start()

      // 主线程阻塞，等待重启时间
      Thread.sleep(restartTime)
      // Query 停止,不再产生新批次
      query.stop()
      // 等待当前批次执行完
      query.awaitTermination()
    &#125;

  &#125;
&#125;
</code></pre>
<h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h1><p>上面实现了两种方式去动态更新参数，从开发的角度来看，第二种开发起来更简单，可控性以及灵活性更强。但是频繁重启query会对性能上有些许影响。第一种实现起来麻烦，但是后期开发使用更简单，无需关心更新逻辑，可移植性比较强，但是只能更新DataFrame，也就是说只适用于实时流数据与静态数据进行join的场景。</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls"
                data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
            <ul id="audio-list" style="display:none">
                
                
                <li title='0' data-url='/statics/Otokaze - Mallow Flower.mp3'></li>
                
                    
            </ul>
            
            
            
        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <img src="http://shirukai.gitee.io/images/a2199f66b2599b9ee3c7bba89fbac4b4.jpg" height=300 width=300></img>
                    <p>shirukai</p>
                    <span>Alway believe that something wonderful is about to happen</span>
                    <dl>
                        <dd><a href="https://github.com/shirukai" target="_blank"><span
                                    class=" iconfont icon-github"></span></a></dd>
                        <dd><a href="" target="_blank"><span
                                    class=" iconfont icon-twitter"></span></a></dd>
                        <dd><a href="" target="_blank"><span
                                    class=" iconfont icon-stack-overflow"></span></a></dd>
                    </dl>
                </div>
                <ul>
                    <li><a href="/">285 <p>Articles</p></a></li>
                    <li><a href="/categories">25 <p>Categories</p></a></li>
                    <li><a href="/tags">46 <p>Tags</p></a></li>
                </ul>
            </div>
            
            
            
            <div class="box sticky animated fadeInRight faster">
                <div id="toc" class="subbox">
                    <h4>Contents</h4>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">1 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E5%9F%BA%E4%BA%8E%E9%87%8D%E5%86%99%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0"><span class="toc-number">2.</span> <span class="toc-text">2 基于重写数据源的动态更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 实现思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Demo%E6%BC%94%E7%A4%BA"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 Demo演示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-%E6%95%B0%E6%8D%AE%E6%8F%8F%E8%BF%B0"><span class="toc-number">2.3.1.</span> <span class="toc-text">2.3.1 数据描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-%E8%87%AA%E5%AE%9A%E4%B9%89KafkaSource"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.3.2 自定义KafkaSource</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%9F%BA%E4%BA%8E%E9%87%8D%E5%90%AFQuery%E7%9A%84%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0"><span class="toc-number">3.</span> <span class="toc-text">3 基于重启Query的动态更新</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 实现思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Demo%E6%BC%94%E7%A4%BA"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 Demo演示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">4 总结</span></a></li></ol>
                </div>
            </div>
            
            
        </div>
    </div>
</div>

    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p class="copyright" id="copyright">
        &copy; 2021
        <span class="gradient-text">
            shirukai
        </span>.
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
        Theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.3" target="_blank" rel="noopener">v1.4.3</a></small>
    </p>
</footer>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX", "TeX"],
      linebreaks: {
        automatic: true
      },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"]
      ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      noUndefined: {
        attributes: {
          mathcolor: "red",
          mathbackground: "#FFEEEE",
          mathsize: "90%"
        }
      },
      Macros: {
        href: "{}"
      }
    },
    messageStyle: "none"
  });
</script>
<script>
  function initialMathJax() {
    MathJax.Hub.Queue(function () {
      var all = MathJax.Hub.getAllJax(),
        i;
      // console.log(all);
      for (i = 0; i < all.length; i += 1) {
        console.log(all[i].SourceElement().parentNode)
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  }

  function reprocessMathJax() {
    if (typeof MathJax !== 'undefined') {
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    }
  }
</script>




<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>


<script src="//cdn.bootcss.com/typed.js/2.0.10/typed.min.js"></script>


<script src="//cdn.bootcss.com/blueimp-md5/2.12.0/js/md5.min.js"></script>


<script src="//cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>


<script src="https://cdn.bootcss.com/codemirror/5.48.4/codemirror.min.js"></script>

    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/javascript/javascript.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/css/css.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/xml/xml.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/clike/clike.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/php/php.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/shell/shell.min.js"></script>


    
<script src="//cdn.bootcss.com/codemirror/5.48.4/mode/python/python.min.js"></script>




    
<script src="/js/busuanzi.min.js"></script>

    <script>
        $(document).ready(function () {
            if ($('span[id^="busuanzi_"]').length) {
                initialBusuanzi();
            }
        });
    </script>



<link rel="stylesheet" href="//cdn.bootcss.com/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="//cdn.bootcss.com/photoswipe/4.1.3/default-skin/default-skin.min.css">


<script src="//cdn.bootcss.com/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="//cdn.bootcss.com/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-149874671-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-149874671-1');
    </script>





<script>
    function initialTyped () {
        var typedTextEl = $('.typed-text');
        if (typedTextEl && typedTextEl.length > 0) {
            var typed = new Typed('.typed-text', {
                strings: ["Alway believe that something wonderful is about to happen", "心之所向，素履以往。"],
                typeSpeed: 90,
                loop: true,
                loopCount: Infinity,
                backSpeed: 20,
            });
        }
    }

    if ($('.article-header') && $('.article-header').length) {
        $(document).ready(function () {
            initialTyped();
        });
    }
</script>




</html>
